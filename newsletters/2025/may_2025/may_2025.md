[![](https://raw.githubusercontent.com/FistOfHit/SixRackUnits/refs/heads/main/assets/header.png)](https://sixrackunits.substack.com)

# May 2025

![](https://raw.githubusercontent.com/FistOfHit/SixRackUnits/refs/heads/main/newsletters/2025/may_2025/images/title.jpeg)

*What if interconnects became zero-latency when it came to distance? What about zero-latency for multiplexing/de-multiplexing, buffering, decoding etc.? Or what if there was no latency between a chip accessing a memory region in any other chip it was working with on a program, regardless of where that other chip is? How would that change compute and memory?*

**Note**: From this issue onwards, the number of one-pagers (segments aimed at explaining a concept in a single page) will be limited to one per month. With an increase in writing on LinkedIn, I will be sharing a lot of content there that will be similar in nature to the one-pagers, and this comes at the cost of having less time to work on this newsletter. I apologise in advance.

[**This month's updates:**](#this-months-updates)
  - [**Nvidia's ASICs for China - post-diffusion act**](#nvidias-asics-for-china-post-diffusion-act)
  - [**Apple reportedly working on the next four generations of chips**](#apple-reportedly-working-on-the-next-four-generations-of-chips)
  - [**Other notable headlines**](#other-notable-headlines)

[**Vendor spotlight:**](#vendor-spotlight)
  - [**iPronics**](#ipronics)

[**One-pager:**](#one-pager)
  - [**Wave Division Multiplexing**](#wave-division-multiplexing)

For a space to share sources and news/updates, join on <a href="https://t.me/aihpc_infra_fans">Telegram</a>, and check out my <a href="https://www.linkedin.com/in/hitesh-kumar58">LinkedIn</a> for posts on similar topics!

---

# This month's updates:

## Nvidia's ASICs for China - post-diffusion act

**

## Apple reportedly working on the next four generations of chips

**

##

**

##

**

##

**


## Other notable headlines

* [Nvidia might postpone the adoption of SOCAMM LPDDR5X modules for its GB300 boards, sources cite thermal issues and stability problems](https://zdnet.co.kr/view/?no=20250514101636)
* [Nvidia's 1.6T optical transceiver's likely delayed from 4Q25 to 1Q26, sources cite supply chain issues](https://www.digitimes.com.tw/tech/dt/n/shwnws.asp?CnlID=1&Cat=40&id=0000721735_JVO6R7644LFNHC4YWMO00)
---

# Vendor spotlight:

## iPronics

*Datacenter switching is reaching a point where the relative power draw of the optical-electrical interfaces (transceivers) required to carry the extreme bandwidths of AI fabrics is becoming a concern. 800G fabrics now need special transceiver casings, as well as taller ports just to stay cool. One way to avoid scaling implementation and integration difficulty is to use optical circuit switching, or OCS.*

![](https://raw.githubusercontent.com/FistOfHit/SixRackUnits/refs/heads/main/newsletters/2025/may_2025/images/ipronics_logo.png)

Founded in 2019, Spanish startup [iPronics](https://ipronics.com/) focusing on "software-defined photonics", or photonics hardware designed to be programmable and reconfigurable. iPronics' OCS technology seeks to replace power hungry and fixed-bandwidth electrical switches in datacenters with photonic (light-based) switches that route optical signals without converting them to electrical signals.

Electrical conversion requires significant amounts of power, maintenance, and specialised hardware to support a limited number bandwidths based on the signal processor design. For example, a 400G switch (referred to as a 25.6T switch by aggregating its 64 ports) requires datacenter staff to use a guide on plugging in the right cable into the right transceiver into the right port, in a particular pattern in order to cable up a particular topology. In addition, these transceivers can only support 400G, 2 x 200G, or 4 x 100G bandwidths usually, and upgrading to higher bandwidths requires new switch ASICs, ports, and transceivers.

![](https://raw.githubusercontent.com/FistOfHit/SixRackUnits/refs/heads/main/newsletters/2025/may_2025/images/ipronics_puc.png)

*Source: iPronics*

Their ["ONE" switch](https://ipronics.com/ipronics-optical-networking-engine/) is a 32-port 1U switch based on their [programmable unit cell](https://ipronics.com/technology/) technology, which is a versatile compute/switching unit that can be combined into structures of seemingly arbitrary size to create something analogous to a switch ASIC with compute attached. Using this, the OCS device can not only route and process signals on the same package, but can also be reconfigured via software to modify the routing and processing within 100s of microseconds. This means that in theory, switches made from these programmable unit cells should be able to support in-network compute as well as rapid topology changes to the fabric without extensive maintenance and costly downtime.

![](https://raw.githubusercontent.com/FistOfHit/SixRackUnits/refs/heads/main/newsletters/2025/may_2025/images/ipronics_one.png)

*Source: iPronics*

Finally, the specs for the ONE switch are as follows:
- 32 ports independent of bandwidth (up to some limits most likely)
- 1U form factor with 8 physical ports it seems
- < 30ns switching latency (compared to 500-1000ns for high end electrical switches)
- < 300 microseconds reconfiguration time compared to 100s of milliseconds for mirror-based optical switches

---

# One-pager:

## Wave Division Multiplexing

**

Wave-division multiplexing, or WDM, is the trick that lets us squeeze vastly more bandwidth out of a single optical fiber than would otherwise be possible. Instead of sending just one stream of data down a fiber, WDM splits the fiber’s capacity into dozens or even hundreds of separate channels, each using a slightly different wavelength of light. Think of it as a prism in reverse: instead of splitting white light into a rainbow, we’re combining a rainbow of tightly controlled laser colors into a single beam, each color carrying its own independent data stream. At the endpoints, a demultiplexer splits the combined light back into its original wavelengths, so each stream can be routed or processed as needed.

The most common flavors you’ll see are CWDM (coarse WDM) and DWDM (dense WDM). CWDM spaces its channels further apart, typically up to 18 per fiber, and is popular for shorter-haul links where cost and simplicity matter more than absolute capacity. DWDM, on the other hand, packs channels much more tightly-up to 192 or more per fiber-making it the backbone of long-haul and hyperscale data center interconnects, where every bit of capacity counts and optical amplification is a must. The upshot is that with WDM, a single fiber that might have once carried 10 Gbps can now carry multiple terabits per second, simply by stacking up enough wavelengths.

This approach is especially critical in modern AI and cloud infrastructure, where the demand for east-west bandwidth between racks, rooms, or even entire data centers has exploded. Rather than digging up roads to lay new fiber every time bandwidth needs double, operators can upgrade their optics and multiplexers, lighting up more wavelengths and scaling capacity with minimal disruption. The real magic is that all these wavelengths coexist on the same glass, passing through the same amplifiers and switches, yet never interfering with each other, so long as their wavelengths are kept sufficiently distinct and the optics are precisely tuned.

WDM is now so fundamental to networking that it’s quietly running behind the scenes in everything from metro fiber rings to the transoceanic cables that tie continents together. And as the appetite for bandwidth keeps growing, the industry keeps pushing for even tighter channel spacing, more sophisticated modulation schemes, and smarter optical hardware-all to keep that single strand of fiber delivering more data, further, and faster than ever before.

[![](https://raw.githubusercontent.com/FistOfHit/SixRackUnits/refs/heads/main/assets/logo.png)](https://sixrackunits.substack.com)
