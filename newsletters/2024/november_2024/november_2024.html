<h1 id="november-2024">November 2024</h1>
<p><img src="./images/1.png" alt=""></p>
<p><em>Server or switch? How many ports is too many?</em></p>
<p><strong>This month&#39;s updates:</strong></p>
<ul>
<li>Meta publishes a re-analysis of reliability in their large-scale ML clusters</li>
<li>Nvidia reportedly rushes SK-Hynix for HBM4, 6 months ahead of schedule</li>
<li>Rumour: Apple considers entering the AI server market via Foxconn and other OEMs</li>
<li>Evidence mounts: AI model quantisation has its limit, and what this means for hardware sales</li>
<li>Lawrence Livermore national labs&#39; El Capitan supercomputer reaches 1st place in the top500</li>
</ul>
<p><strong>One-pagers:</strong></p>
<ul>
<li>GDDR7</li>
<li>SERDES</li>
<li>NUMA</li>
</ul>
<hr>
<h1 id="this-month-s-updates-">This month&#39;s updates:</h1>
<h2 id="meta-s-reliability-analysis">Meta&#39;s reliability analysis</h2>
<p><em>Meta once again demonstrates leadership in tech with a commitment to open-sourcing their analyses by revisiting their <a href="https://research.facebook.com/publications/a-large-scale-study-of-data-center-network-reliability/">previous work</a> on datacentre reliability, this time focusing on AI workloads and GPU clusters. A lot of these findings might already be well known in industry, some are quite surprising.</em></p>
<p><a href="https://arxiv.org/html/2410.21680v1">The report</a> focuses on two of Meta&#39;s largest R&amp;D compute clusters known as <a href="https://ai.meta.com/blog/ai-rsc/">RSC-1</a> (focused on LLM training) and <a href="https://www.servethehome.com/meta-rsc-selects-nvidia-and-pure-storage-for-ai-research-cluster-amd-epyc/meta-rsc-2/">RSC-2</a> (vision model training and other HPC jobs), at 16K and 8K GPUs respectively. Both clusters use InfiniBand for the backend network and have similar network topologies, as well as having the same multiple storage offerings available to users. The paper discusses various methods and experiments and their results in detail.</p>
<p><img src="./images/2.png" alt=""></p>
<p><em>Source: Meta</em></p>
<p>Key observations:</p>
<ul>
<li><a href="https://arxiv.org/html/2410.21680v1#:~:text=we%20see%20that%20such%20failures%20affect%200.2%25%20of%20jobs.%20Nevertheless%2C%20we%20see%20that%2018.7%25%20of%20runtime%20is%20impacted%20by%20these%20failures">[Link]</a> Across all workloads studied, hardware related failures affect only 0.2% of jobs run on the machines but take up 18.7% of their total runtime</li>
<li><a href="https://arxiv.org/html/2410.21680v1#:~:text=Observation%205%3A%20Many%20hardware%20failures%20are%20unattributed%2C%20and%20the%20most%20common%20attributed%20failures%20are%20due%20to%20the%20backend%20network%2C%20the%20filesystem%2C%20and%20GPUs.">[Link]</a> The three largest contributors to failure events were networking (InfiniBand links), GPUs (response timeout), and Storage (random unmounts)</li>
<li><a href="https://arxiv.org/html/2410.21680v1#:~:text=Figure%207%3A,predictably%20with%20scale.">[Link]</a> MTTF (mean time to failure) scales almost linearly with the number of GPUs used by a job, making it easy to predict from past data</li>
<li><a href="https://arxiv.org/html/2410.21680v1#:~:text=TABLE%20I%3A,domains%20are%20suspect.">[Link]</a> There is a reliable failure taxonomy for mapping symptoms to causes, as well as where these failures are likely to show/come from</li>
<li><a href="https://arxiv.org/html/2410.21680v1#:~:text=For%20example%2C%20we%20observe%20PCIe%20errors%20often%20co%2Doccur%20with%20XID%2079%20\(GPU%20falling%20off%20the%20bus\">[Link]</a>%20and%20IPMI%20%E2%80%9CCritical%20Interrupt%E2%80%9D%20events.%20On%20RSC%2D1%20(and%20RSC%2D2)%2C%20we%20observe%2043%25%20(63%25)%20of%20PCI%20errors%20co%2Doccur%20with%20XID%2079%20and%2021%25%20(49%25)%20have%20all%203%20event%20types.) It&#39;s possible to correlate multiple hardware failure types as co-occurring and use these to accelerate fixes. For example, PCIe error rate spikes might indicate GPU errors</li>
<li><a href="https://arxiv.org/html/2410.21680v1#:~:text=Observation%206%3A%20Cluster%20failures%20are%20dynamic%20and%20reducing%20cluster%20failure%20rate%20is%20a%20continuous%20battle.%20New%20workloads%20and%20software%20updates%20mean%20the%20cluster%20is%20constantly%20changing.">[Link]</a> Failure rates (and hence MTTF) change over time for the same system, ML/AI workloads show a clearer diurnal pattern whereas regular HPC jobs do not</li>
</ul>
<p><img src="./images/3.png" alt=""></p>
<p><em>Source: Meta</em></p>
<h2 id="nvidia-rushes-sk-hynix-for-hbm4">Nvidia rushes SK-Hynix for HBM4</h2>
<p><em><a href="https://www.skhynix.com/">SK-Hynix</a>, a leading memory manufacturer, has been asked to accelerate its development and release of <a href="https://www.anandtech.com/show/21088/hbm4-in-development-2048bit-interface-will-require-more-collaboration">HBM4</a> - the next generation of its memory devices that modern datacentre GPUs rely on for performance - <a href="https://www.reuters.com/technology/nvidias-huang-asked-sk-hynix-bring-forward-supply-hbm4-chips-by-6-months-sks-2024-11-04/">by six months</a>. With <a href="https://www.kedglobal.com/korean-chipmakers/newsView/ked202410230012">HBM3&#39;s roadmap</a> not yet complete, will SK-Hynix be able to deliver?</em></p>
<p><a href="https://en.wikipedia.org/wiki/High_Bandwidth_Memory">High-bandwidth memory (HBM)</a> is a key component in all widely deployed AI accelerators currently. Nvidia, AMD, and Intel devices all use HBM to provide their GPUs with enough data to keep utilisation and performance high as <a href="https://www.dataweek.co.za/20539r">regular DDR DRAM or even GDDR just can&#39;t provide the bandwidth</a> to support AI workloads.</p>
<p>SK-Hynix, one of the 3 companies in the world able to produce HBM at scale and currently the market leader, plans to release the 4th generation of this technology in 1H2026. However, with Nvidia wanting to release their next generation of AI GPUs, <a href="https://www.tweaktown.com/news/100151/this-data-center-ai-chip-roadmap-shows-nvidia-will-dominate-far-into-2027-and-beyond/index.html">the Rubin series</a>, in 2H2025, this release would be too late for them to properly qualify and integrate HBM4 instead of HBM3E.</p>
<p><img src="./images/4.png" alt=""></p>
<p><em>Source: AMD</em></p>
<p>HBM3E is currently in its second iteration with the 12-layer version ready to ship in 4Q2024 and the <a href="https://www.trendforce.com/presscenter/news/20241114-12362.html">16-layer version being ready to sample in 1Q2025</a>. With Nvidia&#39;s push for HBM4 before 2026 it&#39;s become clear that they intend to use SK-Hynix&#39;s 12-layer HBM4, which reportedly reaches bandwidths of up to <a href="https://www.eettaiwan.com/20240603ta31-high-bandwidth-memory-hbm-options-for-demanding-compute/">1.5-2TB/s</a> (<a href="https://www.jedec.org/news/pressreleases/jedec-approaches-finalization-hbm4-standard-eyes-future-innovations">JEDEC standard = 1.6TB/s</a>) with a capacity of <a href="https://www.jedec.org/news/pressreleases/jedec-approaches-finalization-hbm4-standard-eyes-future-innovations">up to 64GB per stack</a>. This is a significant improvement from even <a href="https://www.techradar.com/pro/sk-hynix-just-unveiled-the-worlds-largest-capacity-16-layer-hbm3e-chips-at-the-sk-ai-summit">16-layer HBM3E</a> (1.5TB/s and 48GB). Regardless, this still aligns with rumours of the <a href="https://www.tweaktown.com/news/100151/this-data-center-ai-chip-roadmap-shows-nvidia-will-dominate-far-into-2027-and-beyond/index.html">Rubin R100 GPU&#39;s on-chip memory capacity of 576GB</a>.</p>
<p>It&#39;s unclear yet whether any of this affects SK-Hynix&#39;s current plans to <a href="https://www.bnnbloomberg.ca/business/technology/2024/11/04/sk-hynix-speeds-up-release-of-new-ai-chips-at-nvidias-urging/">release HBM5 in 2028-2030</a>. It should also be noted that Samsung currently <a href="https://www.trendforce.com/news/2024/11/12/news-samsung-reportedly-starts-developing-custom-hbm4-for-big-csps-eyeing-mass-production-by-2025/">claims to be releasing their HBM4 in 2H2025</a> also.</p>
<h2 id="rumours-apple-to-build-its-own-datacentre-servers">Rumours: Apple to build its own datacentre servers</h2>
<p><em>As <a href="https://www.apple.com/uk/apple-intelligence/">AI (Apple intelligence)</a> scales to more devices, more capabilities, and faster responses, the volume of servers required to support these inferencing workloads will scale with it. For this, Apple appears to consider <a href="https://asia.nikkei.com/Business/Technology/Tech-Asia/Apple-asks-Foxconn-to-produce-servers-in-Taiwan-in-AI-push">building their own datacentre servers</a> for the first time.</em></p>
<p>According to <a href="https://asia.nikkei.com/Business/Technology/Tech-Asia/Apple-asks-Foxconn-to-produce-servers-in-Taiwan-in-AI-push">reports</a>, Apple has approached Foxconn in Taiwan to manufacture dedicated AI servers <a href="https://www.datacenterdynamics.com/en/news/apple-wants-foxconn-to-manufacture-its-ai-servers-plans-to-use-m4-chips-in-apple-intelligence-servers-from-next-year/">likely based on Apple&#39;s own M4-series processors</a>. These servers will primarily handle AI inferencing workloads to power Apple Intelligence services across their range of products. The design and implementation of such servers differs from cloud providers like Microsoft and Amazon, as Apple&#39;s servers would likely not require intensive power delivery or cooling solutions typical of generally less <a href="https://www.notebookcheck.net/Apple-M4-Pro-analysis-Extremely-fast-but-not-as-efficient.915270.0.html">power efficient</a> and training-focused hardware.</p>
<p>This presents significant challenges for Apple as the company has limited experience in designing and manufacturing such products compared to established players. While focusing on inference workloads might simplify some aspects of the design, the market is still showing unprecedented demand for AI server manufacturing capacity, which could impact Apple&#39;s ability to scale production quickly. The company will need to navigate numerous technical validation issues and manufacturing constraints as it takes on this challenge.</p>
<p><img src="./images/5.png" alt=""></p>
<p>In addition, Foxconn is possibly already at capacity with <a href="https://en.tmtpost.com/post/7276609">their commitments to Nvidia on building a new facility in Mexico</a> that will reportedly produce 20,000 servers a month for Nvidia&#39;s GB200 NVL72 SKU. Given this ongoing scaling of infrastructure, it&#39;s unclear how Foxconn will generate additional capacity for M4 based servers. As a result, <a href="https://www.trendforce.com/news/2024/11/06/news-apple-reportedly-in-talks-with-foxconn-to-produce-ai-servers-in-taiwan/">Apple has also approached Lenovo as well as other OEMs outside of China</a> to ensure supply chain diversification.</p>
<h2 id="scaling-ai-to-become-increasingly-difficult">Scaling AI to become increasingly difficult</h2>
<p><em>Perhaps the most important <a href="https://arxiv.org/pdf/2411.04330">paper</a> in AI this year, a study by a collection of American academics shows that we are approaching <a href="https://buttondown.com/ainews/archive/ainews-bitnet-was-a-lie/">the limits of how much more we can keep quantising</a> or &quot;compressing&quot; AI models. Naturally, this does pose some risk to the growth of future hardware sales.</em></p>
<p><img src="./images/6.png" alt=""></p>
<p>Efficiently inferencing very large AI models relies <a href="https://www.medoid.ai/blog/a-hands-on-walkthrough-on-model-quantization/">on quantising - or &quot;compressing&quot; – their weights</a>. This results in being able to fit the model into fewer AI accelerators and hence <a href="https://arxiv.org/html/2402.09748v1">improve inferencing speed and efficiency</a>. In other words, quantisation allows models to be economically viable – making them suitable for a profitable service to users.</p>
<p>However recent research from academia shows evidence that as models become larger and more data is used to train them, <a href="https://x.com/Tim_Dettmers/status/1856338240099221674">the degree that they can be quantised to has its limits</a>. And those limits are very close to current capabilities. In other words, <a href="https://x.com/Tanishq97836660/status/1856045600355352753">training a larger model on more data will make it more difficult to quantise.</a></p>
<p>The study presented is the first reliable evidence in the field that there are such limits to AI model scaling, and how those limits can be quantified. For AI training labs these results may lead to reconsiderations on how much larger their next generation of models will be based on their available data volumes and compute capacities. Consequently, this will affect how much hardware they buy and use to train such models in the near future.</p>
<h2 id="el-capitan-takes-the-top-spot-in-the-top500">El Capitan takes the top spot in the top500</h2>
<p><em>Announced at SC24, the El Capitan supercomputer achieves <a href="https://www.top500.org/news/el-capitan-achieves-top-spot-frontier-and-aurora-follow-behind/">1st place in the top500 supercomputer rankings.</a> Reaching a peak of 1.742 ExaFLOPS (10^18) on the HPL benchmark, the AMD-powered and HPE-built machine shows significantly more compute capability than its competition.</em></p>
<p>Lawrence Livermore national labs (LLNL) is one of just 11 U.S. public labs to receive more than $1 billion in funding annually and primarily conducts R&amp;D for the department of energy on national security matters. LNLL strategically invests a large amount of that funding into their HPC clusters annually, <a href="https://www.llnl.gov/article/52061/lawrence-livermore-national-laboratorys-el-capitan-verified-worlds-fastest-supercomputer">cumulating in El Capitan</a>, now the most powerful (by FLOPS) supercomputer holding the <a href="https://www.top500.org/lists/top500/2024/11/">top spot in the top500 rankings</a>. The are multiple lists that use various criteria to compare different machines but the primary rankings that the largest machines compete in is by their <a href="https://en.wikipedia.org/wiki/LINPACK_benchmarks">LINPACK score</a>, which measures how many computations the system can perform towards solving a very large generic matrix algebra problem.</p>
<p><img src="./images/7.jpeg" alt=""></p>
<p><em>Source: LLNL</em></p>
<p>Some of the <a href="https://www.top500.org/system/180307/">key features/stats of El Capitan</a>:</p>
<ul>
<li>~11 million cores in total within ~44,000 <a href="https://www.amd.com/en/products/accelerators/instinct/mi300/mi300a.html">AMD MI300A APUs</a> (CPU + GPU compute cores in one chip)</li>
<li>Uses <a href="https://www.servethehome.com/hpe-cray-and-amd-win-again-with-el-capitan-2-exaflop-supercomputer/">HPE-Cray&#39;s Shasta platform</a> for very dense, direct liquid cooled racks</li>
<li>Uses <a href="https://buy.hpe.com/uk/en/options/enterprise-networking-products/switches/slingshot/hpe-slingshot/p/1012904596">HPE Slingshot 11</a> as its back-end data fabric, delivering 200GbE bandwidths per port at servers</li>
<li>Has a combined TDP of 29MW when running this workload, and is designed to reach a peak theoretical draw of 35MW</li>
<li>Reaches 1.74 out of a peak theoretical 2.74 ExaFLOPS in FP64 compute using the HPL benchmark</li>
</ul>
<hr>
<h1 id="one-pagers-">One-pagers:</h1>
<h2 id="gddr7">GDDR7</h2>
<p><em>GDDR7 - the 7th generation of Graphics DDR memory - is now under development. Suited for processors that need large capacity and bandwidth, GDDR7 RAM will continue the competition against HBM.</em></p>
<p><img src="./images/8.jpeg" alt=""></p>
<p><em>Source: Micron</em></p>
<p><img src="./images/9.jpeg" alt=""></p>
<p><em>Source: Samsung</em></p>
<p>Graphics DDR memory was originally developed for gaming and video workloads where regular DDR memory couldn&#39;t provide enough bandwidth to saturate the compute engines of graphics cards well enough. GDDR overcomes this issue by delivering a wide bus width (amount of data that can be transferred at once) at the cost of power and latency. With the explosion in the diversity and utilisation of AI accelerators, GDDR memory has been chosen by some manufacturers as the device memory that strikes the optimal balance between latency, power, and bandwidth.</p>
<p>With DDR (double data rate) memory, transfers can be initiated twice per clock cycle, however DDR7 is referred to technically as QDR (Quad data rate) as it will further double this transfer frequency. Due to being made with a 10 or 12nm process, 7th gen DDR should be significantly higher density then GDDR5 and 6. This will allow for larger capacities - 16-24GBs - as well as significantly higher bandwidths - 32-40Gb/s - per individual bank.</p>
<p>Devices currently expected to use GDDR7 will be primarily gaming GPUs such as Nvidia&#39;s upcoming Blackwell-based RTX 50 series, as well as RDNA4 based AMD GPUs. However, many AI hardware organisations are already using GDDR5/6 and LPDDR5 in their devices. Based on how HBM availability looks in the coming months, it&#39;s possible that GDDR7 will see use in AI accelerators developed in 2025/6 onwards.</p>
<h2 id="serdes">SERDES</h2>
<p><em>A serialiser/deserialiser (SERDES), is a chip that converts digital signals to and from multiple parallel streams and single serial streams. These components are fundamental to modern high-speed data transmission, allowing a complex machine with many communication streams to use relatively few cables/links.</em></p>
<p>SERDES implementations appear in many parts of the hardware stack, handling data movement at multiple levels of the system hierarchy. Within servers, SERDES circuits enable PCIe links between CPUs and other system components such as memory devices. Between servers, SERDES devices are part of the interface between electrical and optical signals, enabling high-bandwidth networks over fibre. In essence, they convert data to and from serial and parallel streams. This enables multiple independent data streams to be sent coherently over the same link, optimising the use of the link by keeping it saturated instead of it being idle.</p>
<p><img src="./images/10.png" alt=""></p>
<p><em>Source: Wikipedia</em></p>
<p>SERDES technology has evolved rapidly to meet increasing bandwidth demands, with each generation roughly doubling data rates and improving power efficiency. Early implementations operated at 56 Gbps, while current state-of-the-art designs achieve 112 Gbps per lane. The latest 224G SERDES implementations are now emerging in 5nm processes, being driven by the market&#39;s push for 800G/1.6T optical transceivers. These varied designs of SERDES circuits support differing reach requirements from very short reach VSR at 0.1m to long reach LR, extending beyond 0.5m. At the limits of SERDES reach and speed, maintaining the quality of the signal becomes difficult or even impossible, and improving signal interpretability at these limits is a very active field of research.</p>
<p>448G SERDES technologies are already in development, though such high speeds present significant signal integrity challenges and will likely require advanced materials and design techniques to maintain viable signal quality across even short-reach interconnects.</p>
<h2 id="numa">NUMA</h2>
<p><em>For a given socket in a modern CPU, some parts of memory will be closer than others, affecting the memory access latency. Non-uniform memory access (NUMA) architectures have emerged to optimise performance by organizing memory accesses based on physical location, becoming vital in modern multi-socket systems.</em></p>
<p>The Intel Cascade lake 6482 dual-socket 40 core CPU (20 cores per socket) is a great study for this, where doing core-core latency benchmarks reveal that when accessing registers from another core on the same socket, the latency is ~50ns, but when doing the same on a core from the other socket, the latency incurred is ~150ns. In practice, for most CPUs the latency increase is around 2-3x, significant for some HPC workloads where multiple small accesses incurring this overhead can cause a large drop in performance.</p>
<p><img src="./images/11.png" alt=""></p>
<p>With NUMA architectures, both the hardware design and the operating system split the CPU into multiple NUMA &quot;nodes&quot;, where core-core communication and the memory allocation is optimised such that processes pinned to certain cores will (if the workload is optimised) communicate more with processes pinned to nearby cores, and allocate memory needed by those processes to locations that are physically close.</p>
<p><img src="./images/12.png" alt=""></p>
<p>This principle applies not only to communication and memory accesses, but also to everything else on the CPU that is asymmetrically distanced from the sockets, such as PCIe sockets, which connect the CPU to network interface cards, AI accelerators, and sometimes even storage. Some CPUs/systems are designed to optimise traffic here too, having PCIe-aware NUMA architectures, which can have even larger impacts on performance.</p>
